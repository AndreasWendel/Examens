{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "e9d57529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b0598086",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"Database/news_data.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9543c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_news_table(Ticker, conn):\n",
    "    \"\"\" \n",
    "    Ticker: str, the name of the table to be created.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {Ticker}_NewsArticles (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            ticker TEXT NOT NULL,\n",
    "            title TEXT NOT NULL,\n",
    "            url TEXT NOT NULL,\n",
    "            time_published TEXT NOT NULL,\n",
    "            closest_time_before_published TEXT NOT NULL,\n",
    "            authors TEXT, -- Stored as JSON string\n",
    "            summary TEXT,\n",
    "            banner_image TEXT,\n",
    "            source TEXT,\n",
    "            category_within_source TEXT,\n",
    "            source_domain TEXT,\n",
    "            topic TEXT, \n",
    "            overall_sentiment_score REAL,\n",
    "            overall_sentiment_label TEXT,\n",
    "            relevance_score REAL, \n",
    "            UNIQUE(ticker, id)\n",
    "        )\n",
    "        \"\"\")\n",
    "    conn.commit()\n",
    "\n",
    "def create_new_price_table(Ticker, conn):\n",
    "    \"\"\" \n",
    "    Ticker: str, the name of the table to be created.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f'''\n",
    "    CREATE TABLE IF NOT EXISTS {Ticker}PriceHistory (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        date TEXT NOT NULL,\n",
    "        open REAL,\n",
    "        high REAL,\n",
    "        low REAL,\n",
    "        close REAL,\n",
    "        volume INTEGER,\n",
    "        ticker TEXT NOT NULL,\n",
    "        sector TEXT NOT NULL,\n",
    "        UNIQUE(ticker, date)\n",
    "    )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "def close_connection(conn):\n",
    "    conn.close()\n",
    "\n",
    "def insert_NewsData(Ticker, data, conn):\n",
    "    \"\"\"\n",
    "    Ticker: str, the name of the table to insert data into.\n",
    "    data: list of tuples, each tuple contains the data to be inserted.\n",
    "    \"\"\"\n",
    "    data.to_sql(f'{Ticker}_NewsArticles', conn, if_exists='append', index=False)\n",
    "    conn.commit()\n",
    "\n",
    "def insert_PriceHistory(Ticker, data, conn):\n",
    "    \"\"\"\n",
    "    Ticker: str, the name of the table to insert data into.\n",
    "    data: Dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    data.to_sql(f'{Ticker}PriceHistory', conn, if_exists='append', index=False)\n",
    "    conn.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93103a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_vantage_news(apikey, ticker, from_date=None, to_date=None, sort='RELEVANCE', limit=10):\n",
    "        \"\"\"\n",
    "        Fetch news using Alpha Vantage API\n",
    "        \n",
    "        Parameters:\n",
    "        keywords (str): Search keywords or company name\n",
    "        from_date (str): Start date in format YYYYMMDD\n",
    "        to_date (str): End date in format YYYYMMDD\n",
    "        \n",
    "        Returns:\n",
    "        dict: News articles and sentiment information\n",
    "        \"\"\"\n",
    "\n",
    "        if not apikey:\n",
    "            print(\"Warning: ALPHA_VANTAGE_API_KEY not found in environment variables\")\n",
    "            return {}\n",
    "        \n",
    "        url = 'https://www.alphavantage.co/query'\n",
    "        params = {\n",
    "            'function': 'NEWS_SENTIMENT',\n",
    "            'tickers': ticker,\n",
    "            'apikey': apikey,\n",
    "            'limit': limit,\n",
    "            'sort': sort\n",
    "        }\n",
    "        from_date = datetime.strptime(from_date, '%Y-%m-%d').strftime('%Y%m%dT%H%M')\n",
    "        to_date = datetime.strptime(to_date, '%Y-%m-%d').strftime('%Y%m%dT%H%M')\n",
    "        if from_date and to_date:\n",
    "            params['time_from'] = from_date\n",
    "            params['time_to'] = to_date\n",
    "                \n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error fetching news: {response.status_code}\")\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca133d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def fetch_10min_tiingo_data(ticker, api_token, sector):\n",
    "    \"\"\"\n",
    "    Fetches 2 years of 10-minute interval historical data for a given stock from Tiingo.\n",
    "    Returns a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    end_date = datetime.date.today()\n",
    "    start_date = end_date - datetime.timedelta(days=730)\n",
    "    interval_days = 60  # ~60 days = safe for 10-min intervals\n",
    "\n",
    "    current = start_date\n",
    "    all_data = []\n",
    "\n",
    "    print(f\"Fetching 10-min data for {ticker} from {start_date} to {end_date}...\")\n",
    "\n",
    "    while current < end_date:\n",
    "        chunk_end = min(current + datetime.timedelta(days=interval_days), end_date)\n",
    "\n",
    "        url = f\"https://api.tiingo.com/iex/{ticker}/prices\"\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        params = {\n",
    "            'startDate': current.isoformat(),\n",
    "            'endDate': chunk_end.isoformat(),\n",
    "            'resampleFreq': '10min',\n",
    "            'columns': 'open,high,low,close,volume',\n",
    "            'token': api_token\n",
    "        }\n",
    "\n",
    "        print(f\"  → Fetching from {current} to {chunk_end}...\")\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            all_data.extend(data)\n",
    "        else:\n",
    "            print(f\"  ❌ Error {response.status_code}: {response.text}\")\n",
    "\n",
    "        current = chunk_end + datetime.timedelta(days=1)\n",
    "        time.sleep(1)  # Add delay to avoid hitting rate limits\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df[\"Ticker\"] = ticker\n",
    "    df[\"sector\"] = sector  # Example sector, replace with actual if needed\n",
    "\n",
    "    if not df.empty:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df.sort_values('date', inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec7a66",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ade8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_price_table(\"AAPL\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d15d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 10-min data for AAPL from 2023-04-24 to 2025-04-23...\n",
      "  → Fetching from 2023-04-24 to 2023-06-23...\n",
      "  → Fetching from 2023-06-24 to 2023-08-23...\n",
      "  → Fetching from 2023-08-24 to 2023-10-23...\n",
      "  → Fetching from 2023-10-24 to 2023-12-23...\n",
      "  → Fetching from 2023-12-24 to 2024-02-22...\n",
      "  → Fetching from 2024-02-23 to 2024-04-23...\n",
      "  → Fetching from 2024-04-24 to 2024-06-23...\n",
      "  → Fetching from 2024-06-24 to 2024-08-23...\n",
      "  → Fetching from 2024-08-24 to 2024-10-23...\n",
      "  → Fetching from 2024-10-24 to 2024-12-23...\n",
      "  → Fetching from 2024-12-24 to 2025-02-22...\n",
      "  → Fetching from 2025-02-23 to 2025-04-23...\n"
     ]
    }
   ],
   "source": [
    "# Replace with your Tiingo API key\n",
    "API_KEY = \"619a6ea449a7a5e044a79c9a2293eee49c339034\"\n",
    "\n",
    "# Fetch for Apple\n",
    "aapl_df = fetch_10min_tiingo_data(\"AAPL\", API_KEY)\n",
    "\n",
    "# Save to CSV\n",
    "aapl_df.to_csv(\"aapl_10min_2yr.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5469e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_PriceHistory(\"AAPL\", aapl_df, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "816809fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_connection(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5305d3fe",
   "metadata": {},
   "source": [
    "ETL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7326e387",
   "metadata": {},
   "source": [
    "Technology\n",
    "AAPL – Apple Inc.\n",
    "\n",
    "MSFT – Microsoft Corp.\n",
    "\n",
    "NVDA – NVIDIA Corp.\n",
    "\n",
    "🏦 Financials\n",
    "JPM – JPMorgan Chase & Co.\n",
    "\n",
    "BAC – Bank of America\n",
    "\n",
    "GS – Goldman Sachs Group Inc.\n",
    "\n",
    "🏘️ Real Estate\n",
    "PLD – Prologis Inc.\n",
    "\n",
    "O – Realty Income Corp.\n",
    "\n",
    "SPG – Simon Property Group Inc.\n",
    "\n",
    "🛢️ Energy\n",
    "XOM – Exxon Mobil Corp.\n",
    "\n",
    "CVX – Chevron Corp.\n",
    "\n",
    "SLB – Schlumberger Limited\n",
    "\n",
    "🛍️ E-commerce / Retail\n",
    "AMZN – Amazon.com Inc.\n",
    "\n",
    "WMT – Walmart Inc.\n",
    "\n",
    "TGT – Target Corp.\n",
    "\n",
    "🍔 Foods and Consumables\n",
    "KO – Coca-Cola Co.\n",
    "\n",
    "PG – Procter & Gamble Co.\n",
    "\n",
    "PEP – PepsiCo Inc.\n",
    "\n",
    "🏥 Healthcare\n",
    "JNJ – Johnson & Johnson\n",
    "\n",
    "PFE – Pfizer Inc.\n",
    "\n",
    "🎬 Entertainment (Optional extras if you want to expand)\n",
    "NFLX – Netflix Inc.\n",
    "\n",
    "DIS – The Walt Disney Company\n",
    "\n",
    "CMCSA – Comcast Corp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "32a5555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Technology': ['AAPL', 'MSFT', 'NVDA'], 'Financials': ['JPM', 'BAC', 'GS'], 'Real Estate': ['PLD', 'O', 'SPG'], 'Energy': ['XOM', 'CVX', 'SLB'], 'E-commerce / Retail': ['AMZN', 'WMT', 'TGT'], 'Foods and Consumables': ['KO', 'PG', 'PEP'], 'Healthcare': ['JNJ', 'PFE'], 'Entertainment': ['NFLX', 'DIS', 'CMCSA']}\n"
     ]
    }
   ],
   "source": [
    "stocks_by_sector = {\n",
    "    \"Technology\": [\"AAPL\", \"MSFT\", \"NVDA\"],\n",
    "    \"Financials\": [\"JPM\", \"BAC\", \"GS\"],\n",
    "    \"Real Estate\": [\"PLD\", \"O\", \"SPG\"],\n",
    "    \"Energy\": [\"XOM\", \"CVX\", \"SLB\"],\n",
    "    \"E-commerce / Retail\": [\"AMZN\", \"WMT\", \"TGT\"],\n",
    "    \"Foods and Consumables\": [\"KO\", \"PG\", \"PEP\"],\n",
    "    \"Healthcare\": [\"JNJ\", \"PFE\"],\n",
    "    \"Entertainment\": [\"NFLX\", \"DIS\", \"CMCSA\"]\n",
    "}\n",
    "\n",
    "print(stocks_by_sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd5c9d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sector: Technology\n",
      "  - MSFT\n",
      "  - NVDA\n",
      "Sector: Financials\n",
      "  - JPM\n",
      "  - BAC\n",
      "  - GS\n",
      "Sector: Real Estate\n",
      "  - PLD\n",
      "  - O\n",
      "  - SPG\n",
      "Sector: Energy\n",
      "  - XOM\n",
      "  - CVX\n",
      "  - SLB\n",
      "Sector: E-commerce / Retail\n",
      "  - AMZN\n",
      "  - WMT\n",
      "  - TGT\n",
      "Sector: Foods and Consumables\n",
      "  - KO\n",
      "  - PG\n",
      "  - PEP\n",
      "Sector: Healthcare\n",
      "  - JNJ\n",
      "  - PFE\n",
      "Sector: Entertainment\n",
      "  - NFLX\n",
      "  - DIS\n",
      "  - CMCSA\n"
     ]
    }
   ],
   "source": [
    "for sector, stocks in stocks_by_sector.items():\n",
    "    print(f\"Sector: {sector}\")\n",
    "    for stock in stocks:\n",
    "        print(f\"  - {stock}\")\n",
    "        create_new_price_table(stock, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_price_table(\"AAPL\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4872248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sector: Technology\n",
      "  Processing stock: MSFT\n",
      "Fetching 10-min data for MSFT from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20280 records for MSFT into the database.\n",
      "  Processing stock: NVDA\n",
      "Fetching 10-min data for NVDA from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20280 records for NVDA into the database.\n",
      "Finished processing sector: Technology\n",
      "Sleeping for 1 hour to avoid rate limits...\n",
      "Resuming...\n",
      "Sector: Financials\n",
      "  Processing stock: JPM\n",
      "Fetching 10-min data for JPM from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20284 records for JPM into the database.\n",
      "  Processing stock: BAC\n",
      "Fetching 10-min data for BAC from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20284 records for BAC into the database.\n",
      "  Processing stock: GS\n",
      "Fetching 10-min data for GS from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20284 records for GS into the database.\n",
      "Finished processing sector: Financials\n",
      "Sleeping for 1 hour to avoid rate limits...\n",
      "Resuming...\n",
      "Sector: Real Estate\n",
      "  Processing stock: PLD\n",
      "Fetching 10-min data for PLD from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20290 records for PLD into the database.\n",
      "  Processing stock: O\n",
      "Fetching 10-min data for O from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20290 records for O into the database.\n",
      "  Processing stock: SPG\n",
      "Fetching 10-min data for SPG from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20290 records for SPG into the database.\n",
      "Finished processing sector: Real Estate\n",
      "Sleeping for 1 hour to avoid rate limits...\n",
      "Resuming...\n",
      "Sector: Energy\n",
      "  Processing stock: XOM\n",
      "Fetching 10-min data for XOM from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20301 records for XOM into the database.\n",
      "  Processing stock: CVX\n",
      "Fetching 10-min data for CVX from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20301 records for CVX into the database.\n",
      "  Processing stock: SLB\n",
      "Fetching 10-min data for SLB from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20301 records for SLB into the database.\n",
      "Finished processing sector: Energy\n",
      "Sleeping for 1 hour to avoid rate limits...\n",
      "Resuming...\n",
      "Sector: E-commerce / Retail\n",
      "  Processing stock: AMZN\n",
      "Fetching 10-min data for AMZN from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20313 records for AMZN into the database.\n",
      "  Processing stock: WMT\n",
      "Fetching 10-min data for WMT from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20313 records for WMT into the database.\n",
      "  Processing stock: TGT\n",
      "Fetching 10-min data for TGT from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20313 records for TGT into the database.\n",
      "Finished processing sector: E-commerce / Retail\n",
      "Sleeping for 1 hour to avoid rate limits...\n",
      "Resuming...\n",
      "Sector: Foods and Consumables\n",
      "  Processing stock: KO\n",
      "Fetching 10-min data for KO from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "  → Fetching from 2024-10-29 to 2024-12-28...\n",
      "  → Fetching from 2024-12-29 to 2025-02-27...\n",
      "  → Fetching from 2025-02-28 to 2025-04-28...\n",
      "  Inserted 20319 records for KO into the database.\n",
      "  Processing stock: PG\n",
      "Fetching 10-min data for PG from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "  → Fetching from 2023-06-29 to 2023-08-28...\n",
      "  → Fetching from 2023-08-29 to 2023-10-28...\n",
      "  → Fetching from 2023-10-29 to 2023-12-28...\n",
      "  → Fetching from 2023-12-29 to 2024-02-27...\n",
      "  → Fetching from 2024-02-28 to 2024-04-28...\n",
      "  → Fetching from 2024-04-29 to 2024-06-28...\n",
      "  → Fetching from 2024-06-29 to 2024-08-28...\n",
      "  → Fetching from 2024-08-29 to 2024-10-28...\n",
      "❌ Error processing PG: HTTPSConnectionPool(host='api.tiingo.com', port=443): Max retries exceeded with url: /iex/PG/prices?startDate=2024-08-29&endDate=2024-10-28&resampleFreq=10min&columns=open%2Chigh%2Clow%2Cclose%2Cvolume&token=619a6ea449a7a5e044a79c9a2293eee49c339034 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000252E12DBED0>: Failed to resolve 'api.tiingo.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  Inserted 20319 records for PG into the database.\n",
      "  Processing stock: PEP\n",
      "Fetching 10-min data for PEP from 2023-04-29 to 2025-04-28...\n",
      "  → Fetching from 2023-04-29 to 2023-06-28...\n",
      "❌ Error processing PEP: HTTPSConnectionPool(host='api.tiingo.com', port=443): Max retries exceeded with url: /iex/PEP/prices?startDate=2023-04-29&endDate=2023-06-28&resampleFreq=10min&columns=open%2Chigh%2Clow%2Cclose%2Cvolume&token=619a6ea449a7a5e044a79c9a2293eee49c339034 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000252E1408F50>: Failed to resolve 'api.tiingo.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  Inserted 20319 records for PEP into the database.\n",
      "Finished processing sector: Foods and Consumables\n",
      "Sleeping for 1 hour to avoid rate limits...\n",
      "Resuming...\n",
      "Sector: Healthcare\n",
      "  Processing stock: JNJ\n",
      "Fetching 10-min data for JNJ from 2023-04-30 to 2025-04-29...\n",
      "  → Fetching from 2023-04-30 to 2023-06-29...\n",
      "  → Fetching from 2023-06-30 to 2023-08-29...\n",
      "  → Fetching from 2023-08-30 to 2023-10-29...\n",
      "  → Fetching from 2023-10-30 to 2023-12-29...\n",
      "  → Fetching from 2023-12-30 to 2024-02-28...\n",
      "  → Fetching from 2024-02-29 to 2024-04-29...\n",
      "  → Fetching from 2024-04-30 to 2024-06-29...\n",
      "  → Fetching from 2024-06-30 to 2024-08-29...\n",
      "  → Fetching from 2024-08-30 to 2024-10-29...\n",
      "  → Fetching from 2024-10-30 to 2024-12-29...\n",
      "  → Fetching from 2024-12-30 to 2025-02-28...\n",
      "  → Fetching from 2025-03-01 to 2025-04-29...\n",
      "  Inserted 20280 records for JNJ into the database.\n",
      "  Processing stock: PFE\n",
      "Fetching 10-min data for PFE from 2023-04-30 to 2025-04-29...\n",
      "  → Fetching from 2023-04-30 to 2023-06-29...\n",
      "  → Fetching from 2023-06-30 to 2023-08-29...\n",
      "  → Fetching from 2023-08-30 to 2023-10-29...\n",
      "  → Fetching from 2023-10-30 to 2023-12-29...\n",
      "  → Fetching from 2023-12-30 to 2024-02-28...\n",
      "  → Fetching from 2024-02-29 to 2024-04-29...\n",
      "  → Fetching from 2024-04-30 to 2024-06-29...\n",
      "  → Fetching from 2024-06-30 to 2024-08-29...\n",
      "  → Fetching from 2024-08-30 to 2024-10-29...\n",
      "  → Fetching from 2024-10-30 to 2024-12-29...\n",
      "  → Fetching from 2024-12-30 to 2025-02-28...\n",
      "  → Fetching from 2025-03-01 to 2025-04-29...\n",
      "  Inserted 20280 records for PFE into the database.\n",
      "Finished processing sector: Healthcare\n",
      "Sleeping for 1 hour to avoid rate limits...\n",
      "Resuming...\n",
      "Sector: Entertainment\n",
      "  Processing stock: NFLX\n",
      "Fetching 10-min data for NFLX from 2023-04-30 to 2025-04-29...\n",
      "  → Fetching from 2023-04-30 to 2023-06-29...\n",
      "  → Fetching from 2023-06-30 to 2023-08-29...\n",
      "  → Fetching from 2023-08-30 to 2023-10-29...\n",
      "  → Fetching from 2023-10-30 to 2023-12-29...\n",
      "  → Fetching from 2023-12-30 to 2024-02-28...\n",
      "  → Fetching from 2024-02-29 to 2024-04-29...\n",
      "  → Fetching from 2024-04-30 to 2024-06-29...\n",
      "  → Fetching from 2024-06-30 to 2024-08-29...\n",
      "  → Fetching from 2024-08-30 to 2024-10-29...\n",
      "  → Fetching from 2024-10-30 to 2024-12-29...\n",
      "  → Fetching from 2024-12-30 to 2025-02-28...\n",
      "  → Fetching from 2025-03-01 to 2025-04-29...\n",
      "  Inserted 20280 records for NFLX into the database.\n",
      "  Processing stock: DIS\n",
      "Fetching 10-min data for DIS from 2023-04-30 to 2025-04-29...\n",
      "  → Fetching from 2023-04-30 to 2023-06-29...\n",
      "  → Fetching from 2023-06-30 to 2023-08-29...\n",
      "  → Fetching from 2023-08-30 to 2023-10-29...\n",
      "  → Fetching from 2023-10-30 to 2023-12-29...\n",
      "  → Fetching from 2023-12-30 to 2024-02-28...\n",
      "  → Fetching from 2024-02-29 to 2024-04-29...\n",
      "  → Fetching from 2024-04-30 to 2024-06-29...\n",
      "  → Fetching from 2024-06-30 to 2024-08-29...\n",
      "  → Fetching from 2024-08-30 to 2024-10-29...\n",
      "  → Fetching from 2024-10-30 to 2024-12-29...\n",
      "  → Fetching from 2024-12-30 to 2025-02-28...\n",
      "  → Fetching from 2025-03-01 to 2025-04-29...\n",
      "  Inserted 20280 records for DIS into the database.\n",
      "  Processing stock: CMCSA\n",
      "Fetching 10-min data for CMCSA from 2023-04-30 to 2025-04-29...\n",
      "  → Fetching from 2023-04-30 to 2023-06-29...\n",
      "  → Fetching from 2023-06-30 to 2023-08-29...\n",
      "  → Fetching from 2023-08-30 to 2023-10-29...\n",
      "  → Fetching from 2023-10-30 to 2023-12-29...\n",
      "  → Fetching from 2023-12-30 to 2024-02-28...\n",
      "  → Fetching from 2024-02-29 to 2024-04-29...\n",
      "  → Fetching from 2024-04-30 to 2024-06-29...\n",
      "  → Fetching from 2024-06-30 to 2024-08-29...\n",
      "  → Fetching from 2024-08-30 to 2024-10-29...\n",
      "  → Fetching from 2024-10-30 to 2024-12-29...\n",
      "  → Fetching from 2024-12-30 to 2025-02-28...\n",
      "  → Fetching from 2025-03-01 to 2025-04-29...\n",
      "  Inserted 20280 records for CMCSA into the database.\n",
      "Finished processing sector: Entertainment\n",
      "Sleeping for 1 hour to avoid rate limits...\n",
      "Resuming...\n",
      "All sectors processed.\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"619a6ea449a7a5e044a79c9a2293eee49c339034\"\n",
    "\n",
    "for sector, stocks in stocks_by_sector.items():\n",
    "    print(f\"Sector: {sector}\")\n",
    "    for stock in stocks:\n",
    "        print(f\"  Processing stock: {stock}\")\n",
    "        try:\n",
    "            stock_df = fetch_10min_tiingo_data(stock, API_KEY, sector)\n",
    "            insert_PriceHistory(stock, stock_df, conn)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {stock}: {e}\")\n",
    "        print(f\"  Inserted {len(stock_df)} records for {stock} into the database.\")\n",
    "    print(f\"Finished processing sector: {sector}\")\n",
    "    print(\"Sleeping for 1 hour to avoid rate limits...\")\n",
    "    time.sleep(3600)  # Add delay to avoid hitting rate limits\n",
    "    print(\"Resuming...\")\n",
    "\n",
    "print(\"All sectors processed.\")\n",
    "close_connection(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a12fde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KO', 'PG', 'PEP']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_by_sector[\"Foods and Consumables\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de849940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sector: Foods and Consumables\n",
      "  Processing stock: PG\n",
      "  Processing stock: PEP\n"
     ]
    }
   ],
   "source": [
    "for sector, stocks in stocks_by_sector.items():\n",
    "    if sector != \"Foods and Consumables\":\n",
    "        continue\n",
    "    print(f\"Sector: {sector}\")\n",
    "    for stock in stocks:\n",
    "        if stock not in (\"PG\", \"PEP\"):\n",
    "            continue                    # Only continue with the next exe if stock is either PG or PEP\n",
    "        print(f\"  Processing stock: {stock}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c83c7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sector: Foods and Consumables\n",
      "  Processing stock: PG\n",
      "Fetching 10-min data for PG from 2023-04-30 to 2025-04-29...\n",
      "  → Fetching from 2023-04-30 to 2023-06-29...\n",
      "  → Fetching from 2023-06-30 to 2023-08-29...\n",
      "  → Fetching from 2023-08-30 to 2023-10-29...\n",
      "  → Fetching from 2023-10-30 to 2023-12-29...\n",
      "  → Fetching from 2023-12-30 to 2024-02-28...\n",
      "  → Fetching from 2024-02-29 to 2024-04-29...\n",
      "  → Fetching from 2024-04-30 to 2024-06-29...\n",
      "  → Fetching from 2024-06-30 to 2024-08-29...\n",
      "  → Fetching from 2024-08-30 to 2024-10-29...\n",
      "  → Fetching from 2024-10-30 to 2024-12-29...\n",
      "  → Fetching from 2024-12-30 to 2025-02-28...\n",
      "  → Fetching from 2025-03-01 to 2025-04-29...\n",
      "  Inserted 20291 records for PG into the database.\n",
      "  Processing stock: PEP\n",
      "Fetching 10-min data for PEP from 2023-04-30 to 2025-04-29...\n",
      "  → Fetching from 2023-04-30 to 2023-06-29...\n",
      "  → Fetching from 2023-06-30 to 2023-08-29...\n",
      "  → Fetching from 2023-08-30 to 2023-10-29...\n",
      "  → Fetching from 2023-10-30 to 2023-12-29...\n",
      "  → Fetching from 2023-12-30 to 2024-02-28...\n",
      "  → Fetching from 2024-02-29 to 2024-04-29...\n",
      "  → Fetching from 2024-04-30 to 2024-06-29...\n",
      "  → Fetching from 2024-06-30 to 2024-08-29...\n",
      "  → Fetching from 2024-08-30 to 2024-10-29...\n",
      "  → Fetching from 2024-10-30 to 2024-12-29...\n",
      "  → Fetching from 2024-12-30 to 2025-02-28...\n",
      "  → Fetching from 2025-03-01 to 2025-04-29...\n",
      "  Inserted 20291 records for PEP into the database.\n",
      "Finished processing sector: Foods and Consumables\n",
      "All sectors processed.\n"
     ]
    }
   ],
   "source": [
    "safty_list = []\n",
    "\n",
    "for sector, stocks in stocks_by_sector.items():\n",
    "    if sector != \"Foods and Consumables\":\n",
    "        continue\n",
    "    print(f\"Sector: {sector}\")\n",
    "    for stock in stocks:\n",
    "        if stock not in (\"PG\", \"PEP\"):\n",
    "            continue\n",
    "        print(f\"  Processing stock: {stock}\")\n",
    "        try:\n",
    "            stock_df = fetch_10min_tiingo_data(stock, API_KEY, sector)\n",
    "            safty_list.append(stock_df)  # Append the DataFrame to the list\n",
    "            insert_PriceHistory(stock, stock_df, conn)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {stock}: {e}\")\n",
    "        print(f\"  Inserted {len(stock_df)} records for {stock} into the database.\")\n",
    "    print(f\"Finished processing sector: {sector}\")\n",
    "print(\"All sectors processed.\")\n",
    "close_connection(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d68a5",
   "metadata": {},
   "source": [
    "Test for news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "fb4677a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sector: Technology\n",
      "  Creating news table for: AAPL\n",
      "  Creating news table for: MSFT\n",
      "  Creating news table for: NVDA\n",
      "Sector: Financials\n",
      "  Creating news table for: JPM\n",
      "  Creating news table for: BAC\n",
      "  Creating news table for: GS\n",
      "Sector: Real Estate\n",
      "  Creating news table for: PLD\n",
      "  Creating news table for: O\n",
      "  Creating news table for: SPG\n",
      "Sector: Energy\n",
      "  Creating news table for: XOM\n",
      "  Creating news table for: CVX\n",
      "  Creating news table for: SLB\n",
      "Sector: E-commerce / Retail\n",
      "  Creating news table for: AMZN\n",
      "  Creating news table for: WMT\n",
      "  Creating news table for: TGT\n",
      "Sector: Foods and Consumables\n",
      "  Creating news table for: KO\n",
      "  Creating news table for: PG\n",
      "  Creating news table for: PEP\n",
      "Sector: Healthcare\n",
      "  Creating news table for: JNJ\n",
      "  Creating news table for: PFE\n",
      "Sector: Entertainment\n",
      "  Creating news table for: NFLX\n",
      "  Creating news table for: DIS\n",
      "  Creating news table for: CMCSA\n",
      "All news tables created.\n"
     ]
    }
   ],
   "source": [
    "for sector, stocks in stocks_by_sector.items():\n",
    "    print(f\"Sector: {sector}\")\n",
    "    for stock in stocks:\n",
    "        print(f\"  Creating news table for: {stock}\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS {stock}_NewsArticles\")\n",
    "        create_new_news_table(stock, conn)\n",
    "    \n",
    "print(\"All news tables created.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f1603cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_news = get_alpha_vantage_news(apikey='R6WUON3CB6JRP1K7', ticker='AAPL', from_date='2023-01-01', to_date='2023-01-02', limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d591858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alpha_news['feed'])  # Check the length of the feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d67674e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'The Year Big Tech Stocks Fell From Glory',\n",
       "  'url': 'https://www.wsj.com/articles/the-year-big-tech-stocks-fell-from-glory-11672543952',\n",
       "  'time_published': '20230101T103000',\n",
       "  'authors': ['Hannah Miao'],\n",
       "  'summary': 'With interest rates rising sharply, momentum shifted in favor of defensive plays, while the energy sector rallied amid geopolitical strife.',\n",
       "  'banner_image': 'https://images.wsj.net/im-695150/social',\n",
       "  'source': 'Wall Street Journal',\n",
       "  'category_within_source': 'Markets',\n",
       "  'source_domain': 'www.wsj.com',\n",
       "  'topics': [{'topic': 'Technology', 'relevance_score': '1.0'},\n",
       "   {'topic': 'Financial Markets', 'relevance_score': '0.360215'}],\n",
       "  'overall_sentiment_score': 0.332214,\n",
       "  'overall_sentiment_label': 'Somewhat-Bullish',\n",
       "  'ticker_sentiment': [{'ticker': 'NFLX',\n",
       "    'relevance_score': '0.436009',\n",
       "    'ticker_sentiment_score': '0.357976',\n",
       "    'ticker_sentiment_label': 'Bullish'},\n",
       "   {'ticker': 'GOOG',\n",
       "    'relevance_score': '0.751437',\n",
       "    'ticker_sentiment_score': '0.466083',\n",
       "    'ticker_sentiment_label': 'Bullish'},\n",
       "   {'ticker': 'META',\n",
       "    'relevance_score': '0.751437',\n",
       "    'ticker_sentiment_score': '0.466083',\n",
       "    'ticker_sentiment_label': 'Bullish'},\n",
       "   {'ticker': 'AAPL',\n",
       "    'relevance_score': '0.436009',\n",
       "    'ticker_sentiment_score': '0.357976',\n",
       "    'ticker_sentiment_label': 'Bullish'}]},\n",
       " {'title': '2 Beaten-Down Warren Buffett Stocks to Buy in 2023',\n",
       "  'url': 'https://www.fool.com/investing/2023/01/01/beaten-down-warren-buffett-stocks-to-buy-in-2023/',\n",
       "  'time_published': '20230101T101300',\n",
       "  'authors': ['Cory Renauer'],\n",
       "  'summary': \"These are high-conviction stocks for the Oracle of Omaha but they're trading at knocked-down prices.\",\n",
       "  'banner_image': 'https://g.foolcdn.com/image/?url=https%3A%2F%2Fg.foolcdn.com%2Feditorial%2Fimages%2F714673%2Fbitcoin-price-chart-cryptocurrency-ethereum-ripple-getty.jpg&op=resize&h=340',\n",
       "  'source': 'Motley Fool',\n",
       "  'category_within_source': 'n/a',\n",
       "  'source_domain': 'www.fool.com',\n",
       "  'topics': [{'topic': 'Financial Markets', 'relevance_score': '0.5855'},\n",
       "   {'topic': 'Earnings', 'relevance_score': '0.650727'},\n",
       "   {'topic': 'Technology', 'relevance_score': '0.5'},\n",
       "   {'topic': 'Finance', 'relevance_score': '0.5'}],\n",
       "  'overall_sentiment_score': 0.271653,\n",
       "  'overall_sentiment_label': 'Somewhat-Bullish',\n",
       "  'ticker_sentiment': [{'ticker': 'NFLX',\n",
       "    'relevance_score': '0.05654',\n",
       "    'ticker_sentiment_score': '0.067098',\n",
       "    'ticker_sentiment_label': 'Neutral'},\n",
       "   {'ticker': 'AAPL',\n",
       "    'relevance_score': '0.277118',\n",
       "    'ticker_sentiment_score': '0.187644',\n",
       "    'ticker_sentiment_label': 'Somewhat-Bullish'},\n",
       "   {'ticker': 'PARA',\n",
       "    'relevance_score': '0.277118',\n",
       "    'ticker_sentiment_score': '0.222256',\n",
       "    'ticker_sentiment_label': 'Somewhat-Bullish'},\n",
       "   {'ticker': 'BRK-A',\n",
       "    'relevance_score': '0.112797',\n",
       "    'ticker_sentiment_score': '0.138191',\n",
       "    'ticker_sentiment_label': 'Neutral'}]},\n",
       " {'title': \"80% of Warren Buffett's Portfolio Is Invested in These 7 Stocks as 2023 Begins\",\n",
       "  'url': 'https://www.fool.com/investing/2023/01/01/80-of-warren-buffetts-portfolio-is-invested-in-the/',\n",
       "  'time_published': '20230101T105000',\n",
       "  'authors': ['Keith Speights'],\n",
       "  'summary': 'Two of these stocks were especially big winners for Buffett in 2022.',\n",
       "  'banner_image': 'https://g.foolcdn.com/image/?url=https%3A%2F%2Fg.foolcdn.com%2Feditorial%2Fimages%2F714673%2Fbitcoin-price-chart-cryptocurrency-ethereum-ripple-getty.jpg&op=resize&h=340',\n",
       "  'source': 'Motley Fool',\n",
       "  'category_within_source': 'n/a',\n",
       "  'source_domain': 'www.fool.com',\n",
       "  'topics': [{'topic': 'Financial Markets', 'relevance_score': '0.54554'},\n",
       "   {'topic': 'Manufacturing', 'relevance_score': '0.25'},\n",
       "   {'topic': 'Energy & Transportation', 'relevance_score': '0.25'},\n",
       "   {'topic': 'Technology', 'relevance_score': '0.25'},\n",
       "   {'topic': 'Finance', 'relevance_score': '0.25'}],\n",
       "  'overall_sentiment_score': 0.232526,\n",
       "  'overall_sentiment_label': 'Somewhat-Bullish',\n",
       "  'ticker_sentiment': [{'ticker': 'OXY',\n",
       "    'relevance_score': '0.119235',\n",
       "    'ticker_sentiment_score': '0.191777',\n",
       "    'ticker_sentiment_label': 'Somewhat-Bullish'},\n",
       "   {'ticker': 'AAPL',\n",
       "    'relevance_score': '0.178021',\n",
       "    'ticker_sentiment_score': '-0.066673',\n",
       "    'ticker_sentiment_label': 'Neutral'},\n",
       "   {'ticker': 'BAC',\n",
       "    'relevance_score': '0.119235',\n",
       "    'ticker_sentiment_score': '0.211746',\n",
       "    'ticker_sentiment_label': 'Somewhat-Bullish'},\n",
       "   {'ticker': 'CVX',\n",
       "    'relevance_score': '0.178021',\n",
       "    'ticker_sentiment_score': '0.229812',\n",
       "    'ticker_sentiment_label': 'Somewhat-Bullish'},\n",
       "   {'ticker': 'KO',\n",
       "    'relevance_score': '0.178021',\n",
       "    'ticker_sentiment_score': '-0.008809',\n",
       "    'ticker_sentiment_label': 'Neutral'},\n",
       "   {'ticker': 'KHC',\n",
       "    'relevance_score': '0.235823',\n",
       "    'ticker_sentiment_score': '0.125614',\n",
       "    'ticker_sentiment_label': 'Neutral'},\n",
       "   {'ticker': 'AXP',\n",
       "    'relevance_score': '0.119235',\n",
       "    'ticker_sentiment_score': '0.192681',\n",
       "    'ticker_sentiment_label': 'Somewhat-Bullish'}]}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_news['feed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "52ba1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20230101T103000', '20230101T101300', '20230101T105000']\n"
     ]
    }
   ],
   "source": [
    "time_published_list = [item['time_published'] for item in alpha_news[\"feed\"] if 'time_published' in item]\n",
    "print(time_published_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "285bc0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20230101': ['20230101T103000', '20230101T101300', '20230101T105000']}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group timestamps by date\n",
    "grouped_by_date = defaultdict(list)\n",
    "for timestamp in time_published_list:\n",
    "    date = timestamp[:8]  # Extract the date portion (YYYYMMDD)\n",
    "    grouped_by_date[date].append(timestamp)\n",
    "\n",
    "# Convert defaultdict to a regular dictionary for better readability\n",
    "grouped_by_date = dict(grouped_by_date)\n",
    "\n",
    "print(grouped_by_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "3f3c18c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230101\n"
     ]
    }
   ],
   "source": [
    "for name,dict_ in grouped_by_date.items():\n",
    "    print(f\"{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "be3f5728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the name of the dictionary is  title\n",
      "the dictionary looks like  The Year Big Tech Stocks Fell From Glory\n",
      "the dictionary type is  <class 'str'>\n",
      "the name of the dictionary is  url\n",
      "the dictionary looks like  https://www.wsj.com/articles/the-year-big-tech-stocks-fell-from-glory-11672543952\n",
      "the dictionary type is  <class 'str'>\n",
      "the name of the dictionary is  time_published\n",
      "the dictionary looks like  20230101T103000\n",
      "the dictionary type is  <class 'str'>\n",
      "the name of the dictionary is  authors\n",
      "the dictionary looks like  ['Hannah Miao']\n",
      "the dictionary type is  <class 'list'>\n",
      "the name of the dictionary is  summary\n",
      "the dictionary looks like  With interest rates rising sharply, momentum shifted in favor of defensive plays, while the energy sector rallied amid geopolitical strife.\n",
      "the dictionary type is  <class 'str'>\n",
      "the name of the dictionary is  banner_image\n",
      "the dictionary looks like  https://images.wsj.net/im-695150/social\n",
      "the dictionary type is  <class 'str'>\n",
      "the name of the dictionary is  source\n",
      "the dictionary looks like  Wall Street Journal\n",
      "the dictionary type is  <class 'str'>\n",
      "the name of the dictionary is  category_within_source\n",
      "the dictionary looks like  Markets\n",
      "the dictionary type is  <class 'str'>\n",
      "the name of the dictionary is  source_domain\n",
      "the dictionary looks like  www.wsj.com\n",
      "the dictionary type is  <class 'str'>\n",
      "the name of the dictionary is  topics\n",
      "the dictionary looks like  [{'topic': 'Technology', 'relevance_score': '1.0'}, {'topic': 'Financial Markets', 'relevance_score': '0.360215'}]\n",
      "the dictionary type is  <class 'list'>\n",
      "the name of the dictionary is  overall_sentiment_score\n",
      "the dictionary looks like  0.332214\n",
      "the dictionary type is  <class 'float'>\n",
      "the name of the dictionary is  overall_sentiment_label\n",
      "the dictionary looks like  Somewhat-Bullish\n",
      "the dictionary type is  <class 'str'>\n",
      "the name of the dictionary is  ticker_sentiment\n",
      "the dictionary looks like  [{'ticker': 'NFLX', 'relevance_score': '0.436009', 'ticker_sentiment_score': '0.357976', 'ticker_sentiment_label': 'Bullish'}, {'ticker': 'GOOG', 'relevance_score': '0.751437', 'ticker_sentiment_score': '0.466083', 'ticker_sentiment_label': 'Bullish'}, {'ticker': 'META', 'relevance_score': '0.751437', 'ticker_sentiment_score': '0.466083', 'ticker_sentiment_label': 'Bullish'}, {'ticker': 'AAPL', 'relevance_score': '0.436009', 'ticker_sentiment_score': '0.357976', 'ticker_sentiment_label': 'Bullish'}]\n",
      "the dictionary type is  <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for name,dict_ in alpha_news[\"feed\"][0].items():\n",
    "    print ('the name of the dictionary is ', name)\n",
    "    print ('the dictionary looks like ', dict_)\n",
    "    print ('the dictionary type is ', type(dict_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "03a6ffc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'The Year Big Tech Stocks Fell From Glory',\n",
       "  'url': 'https://www.wsj.com/articles/the-year-big-tech-stocks-fell-from-glory-11672543952',\n",
       "  'time_published': '20230101T103000',\n",
       "  'authors': ['Hannah Miao'],\n",
       "  'summary': 'With interest rates rising sharply, momentum shifted in favor of defensive plays, while the energy sector rallied amid geopolitical strife.',\n",
       "  'banner_image': 'https://images.wsj.net/im-695150/social',\n",
       "  'source': 'Wall Street Journal',\n",
       "  'category_within_source': 'Markets',\n",
       "  'source_domain': 'www.wsj.com',\n",
       "  'topics': [{'topic': 'Technology', 'relevance_score': '1.0'},\n",
       "   {'topic': 'Financial Markets', 'relevance_score': '0.360215'}],\n",
       "  'overall_sentiment_score': 0.332214,\n",
       "  'overall_sentiment_label': 'Somewhat-Bullish',\n",
       "  'ticker_sentiment': [{'ticker': 'NFLX',\n",
       "    'relevance_score': '0.436009',\n",
       "    'ticker_sentiment_score': '0.357976',\n",
       "    'ticker_sentiment_label': 'Bullish'},\n",
       "   {'ticker': 'GOOG',\n",
       "    'relevance_score': '0.751437',\n",
       "    'ticker_sentiment_score': '0.466083',\n",
       "    'ticker_sentiment_label': 'Bullish'},\n",
       "   {'ticker': 'META',\n",
       "    'relevance_score': '0.751437',\n",
       "    'ticker_sentiment_score': '0.466083',\n",
       "    'ticker_sentiment_label': 'Bullish'},\n",
       "   {'ticker': 'AAPL',\n",
       "    'relevance_score': '0.436009',\n",
       "    'ticker_sentiment_score': '0.357976',\n",
       "    'ticker_sentiment_label': 'Bullish'}]},\n",
       " {'title': '2 Beaten-Down Warren Buffett Stocks to Buy in 2023',\n",
       "  'url': 'https://www.fool.com/investing/2023/01/01/beaten-down-warren-buffett-stocks-to-buy-in-2023/',\n",
       "  'time_published': '20230101T101300',\n",
       "  'authors': ['Cory Renauer'],\n",
       "  'summary': \"These are high-conviction stocks for the Oracle of Omaha but they're trading at knocked-down prices.\",\n",
       "  'banner_image': 'https://g.foolcdn.com/image/?url=https%3A%2F%2Fg.foolcdn.com%2Feditorial%2Fimages%2F714673%2Fbitcoin-price-chart-cryptocurrency-ethereum-ripple-getty.jpg&op=resize&h=340',\n",
       "  'source': 'Motley Fool',\n",
       "  'category_within_source': 'n/a',\n",
       "  'source_domain': 'www.fool.com',\n",
       "  'topics': [{'topic': 'Financial Markets', 'relevance_score': '0.5855'},\n",
       "   {'topic': 'Earnings', 'relevance_score': '0.650727'},\n",
       "   {'topic': 'Technology', 'relevance_score': '0.5'},\n",
       "   {'topic': 'Finance', 'relevance_score': '0.5'}],\n",
       "  'overall_sentiment_score': 0.271653,\n",
       "  'overall_sentiment_label': 'Somewhat-Bullish',\n",
       "  'ticker_sentiment': [{'ticker': 'NFLX',\n",
       "    'relevance_score': '0.05654',\n",
       "    'ticker_sentiment_score': '0.067098',\n",
       "    'ticker_sentiment_label': 'Neutral'},\n",
       "   {'ticker': 'AAPL',\n",
       "    'relevance_score': '0.277118',\n",
       "    'ticker_sentiment_score': '0.187644',\n",
       "    'ticker_sentiment_label': 'Somewhat-Bullish'},\n",
       "   {'ticker': 'PARA',\n",
       "    'relevance_score': '0.277118',\n",
       "    'ticker_sentiment_score': '0.222256',\n",
       "    'ticker_sentiment_label': 'Somewhat-Bullish'},\n",
       "   {'ticker': 'BRK-A',\n",
       "    'relevance_score': '0.112797',\n",
       "    'ticker_sentiment_score': '0.138191',\n",
       "    'ticker_sentiment_label': 'Neutral'}]},\n",
       " {'title': \"80% of Warren Buffett's Portfolio Is Invested in These 7 Stocks as 2023 Begins\",\n",
       "  'url': 'https://www.fool.com/investing/2023/01/01/80-of-warren-buffetts-portfolio-is-invested-in-the/',\n",
       "  'time_published': '20230101T105000',\n",
       "  'authors': ['Keith Speights'],\n",
       "  'summary': 'Two of these stocks were especially big winners for Buffett in 2022.',\n",
       "  'banner_image': 'https://g.foolcdn.com/image/?url=https%3A%2F%2Fg.foolcdn.com%2Feditorial%2Fimages%2F714673%2Fbitcoin-price-chart-cryptocurrency-ethereum-ripple-getty.jpg&op=resize&h=340',\n",
       "  'source': 'Motley Fool',\n",
       "  'category_within_source': 'n/a',\n",
       "  'source_domain': 'www.fool.com',\n",
       "  'topics': [{'topic': 'Financial Markets', 'relevance_score': '0.54554'},\n",
       "   {'topic': 'Manufacturing', 'relevance_score': '0.25'},\n",
       "   {'topic': 'Energy & Transportation', 'relevance_score': '0.25'},\n",
       "   {'topic': 'Technology', 'relevance_score': '0.25'},\n",
       "   {'topic': 'Finance', 'relevance_score': '0.25'}],\n",
       "  'overall_sentiment_score': 0.232526,\n",
       "  'overall_sentiment_label': 'Somewhat-Bullish',\n",
       "  'ticker_sentiment': [{'ticker': 'OXY',\n",
       "    'relevance_score': '0.119235',\n",
       "    'ticker_sentiment_score': '0.191777',\n",
       "    'ticker_sentiment_label': 'Somewhat-Bullish'},\n",
       "   {'ticker': 'AAPL',\n",
       "    'relevance_score': '0.178021',\n",
       "    'ticker_sentiment_score': '-0.066673',\n",
       "    'ticker_sentiment_label': 'Neutral'},\n",
       "   {'ticker': 'BAC',\n",
       "    'relevance_score': '0.119235',\n",
       "    'ticker_sentiment_score': '0.211746',\n",
       "    'ticker_sentiment_label': 'Somewhat-Bullish'},\n",
       "   {'ticker': 'CVX',\n",
       "    'relevance_score': '0.178021',\n",
       "    'ticker_sentiment_score': '0.229812',\n",
       "    'ticker_sentiment_label': 'Somewhat-Bullish'},\n",
       "   {'ticker': 'KO',\n",
       "    'relevance_score': '0.178021',\n",
       "    'ticker_sentiment_score': '-0.008809',\n",
       "    'ticker_sentiment_label': 'Neutral'},\n",
       "   {'ticker': 'KHC',\n",
       "    'relevance_score': '0.235823',\n",
       "    'ticker_sentiment_score': '0.125614',\n",
       "    'ticker_sentiment_label': 'Neutral'},\n",
       "   {'ticker': 'AXP',\n",
       "    'relevance_score': '0.119235',\n",
       "    'ticker_sentiment_score': '0.192681',\n",
       "    'ticker_sentiment_label': 'Somewhat-Bullish'}]}]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_news[\"feed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "61f38cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.DataFrame([alpha_news[\"feed\"][0]])\n",
    "for i in range(1, len(alpha_news[\"feed\"])):\n",
    "    news1 = pd.DataFrame([alpha_news[\"feed\"][i]])\n",
    "    news = pd.concat([news, news1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "d3e96b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>time_published</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>banner_image</th>\n",
       "      <th>source</th>\n",
       "      <th>category_within_source</th>\n",
       "      <th>source_domain</th>\n",
       "      <th>topics</th>\n",
       "      <th>overall_sentiment_score</th>\n",
       "      <th>overall_sentiment_label</th>\n",
       "      <th>ticker_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Year Big Tech Stocks Fell From Glory</td>\n",
       "      <td>https://www.wsj.com/articles/the-year-big-tech...</td>\n",
       "      <td>20230101T103000</td>\n",
       "      <td>[Hannah Miao]</td>\n",
       "      <td>With interest rates rising sharply, momentum s...</td>\n",
       "      <td>https://images.wsj.net/im-695150/social</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Markets</td>\n",
       "      <td>www.wsj.com</td>\n",
       "      <td>[{'topic': 'Technology', 'relevance_score': '1...</td>\n",
       "      <td>0.332214</td>\n",
       "      <td>Somewhat-Bullish</td>\n",
       "      <td>[{'ticker': 'NFLX', 'relevance_score': '0.4360...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 Beaten-Down Warren Buffett Stocks to Buy in ...</td>\n",
       "      <td>https://www.fool.com/investing/2023/01/01/beat...</td>\n",
       "      <td>20230101T101300</td>\n",
       "      <td>[Cory Renauer]</td>\n",
       "      <td>These are high-conviction stocks for the Oracl...</td>\n",
       "      <td>https://g.foolcdn.com/image/?url=https%3A%2F%2...</td>\n",
       "      <td>Motley Fool</td>\n",
       "      <td>n/a</td>\n",
       "      <td>www.fool.com</td>\n",
       "      <td>[{'topic': 'Financial Markets', 'relevance_sco...</td>\n",
       "      <td>0.271653</td>\n",
       "      <td>Somewhat-Bullish</td>\n",
       "      <td>[{'ticker': 'NFLX', 'relevance_score': '0.0565...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80% of Warren Buffett's Portfolio Is Invested ...</td>\n",
       "      <td>https://www.fool.com/investing/2023/01/01/80-o...</td>\n",
       "      <td>20230101T105000</td>\n",
       "      <td>[Keith Speights]</td>\n",
       "      <td>Two of these stocks were especially big winner...</td>\n",
       "      <td>https://g.foolcdn.com/image/?url=https%3A%2F%2...</td>\n",
       "      <td>Motley Fool</td>\n",
       "      <td>n/a</td>\n",
       "      <td>www.fool.com</td>\n",
       "      <td>[{'topic': 'Financial Markets', 'relevance_sco...</td>\n",
       "      <td>0.232526</td>\n",
       "      <td>Somewhat-Bullish</td>\n",
       "      <td>[{'ticker': 'OXY', 'relevance_score': '0.11923...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0           The Year Big Tech Stocks Fell From Glory   \n",
       "1  2 Beaten-Down Warren Buffett Stocks to Buy in ...   \n",
       "2  80% of Warren Buffett's Portfolio Is Invested ...   \n",
       "\n",
       "                                                 url   time_published  \\\n",
       "0  https://www.wsj.com/articles/the-year-big-tech...  20230101T103000   \n",
       "1  https://www.fool.com/investing/2023/01/01/beat...  20230101T101300   \n",
       "2  https://www.fool.com/investing/2023/01/01/80-o...  20230101T105000   \n",
       "\n",
       "            authors                                            summary  \\\n",
       "0     [Hannah Miao]  With interest rates rising sharply, momentum s...   \n",
       "1    [Cory Renauer]  These are high-conviction stocks for the Oracl...   \n",
       "2  [Keith Speights]  Two of these stocks were especially big winner...   \n",
       "\n",
       "                                        banner_image               source  \\\n",
       "0            https://images.wsj.net/im-695150/social  Wall Street Journal   \n",
       "1  https://g.foolcdn.com/image/?url=https%3A%2F%2...          Motley Fool   \n",
       "2  https://g.foolcdn.com/image/?url=https%3A%2F%2...          Motley Fool   \n",
       "\n",
       "  category_within_source source_domain  \\\n",
       "0                Markets   www.wsj.com   \n",
       "1                    n/a  www.fool.com   \n",
       "2                    n/a  www.fool.com   \n",
       "\n",
       "                                              topics  overall_sentiment_score  \\\n",
       "0  [{'topic': 'Technology', 'relevance_score': '1...                 0.332214   \n",
       "1  [{'topic': 'Financial Markets', 'relevance_sco...                 0.271653   \n",
       "2  [{'topic': 'Financial Markets', 'relevance_sco...                 0.232526   \n",
       "\n",
       "  overall_sentiment_label                                   ticker_sentiment  \n",
       "0        Somewhat-Bullish  [{'ticker': 'NFLX', 'relevance_score': '0.4360...  \n",
       "1        Somewhat-Bullish  [{'ticker': 'NFLX', 'relevance_score': '0.0565...  \n",
       "2        Somewhat-Bullish  [{'ticker': 'OXY', 'relevance_score': '0.11923...  "
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "157acff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Year Big Tech Stocks Fell From Glory\n",
      "<class 'str'>\n",
      "yes\n",
      "------------------\n",
      "https://www.wsj.com/articles/the-year-big-tech-stocks-fell-from-glory-11672543952\n",
      "<class 'str'>\n",
      "yes\n",
      "------------------\n",
      "20230101T103000\n",
      "<class 'str'>\n",
      "yes\n",
      "------------------\n",
      "['Hannah Miao']\n",
      "<class 'list'>\n",
      "no\n",
      "------------------\n",
      "With interest rates rising sharply, momentum shifted in favor of defensive plays, while the energy sector rallied amid geopolitical strife.\n",
      "<class 'str'>\n",
      "yes\n",
      "------------------\n",
      "https://images.wsj.net/im-695150/social\n",
      "<class 'str'>\n",
      "yes\n",
      "------------------\n",
      "Wall Street Journal\n",
      "<class 'str'>\n",
      "yes\n",
      "------------------\n",
      "Markets\n",
      "<class 'str'>\n",
      "yes\n",
      "------------------\n",
      "www.wsj.com\n",
      "<class 'str'>\n",
      "yes\n",
      "------------------\n",
      "[{'topic': 'Technology', 'relevance_score': '1.0'}, {'topic': 'Financial Markets', 'relevance_score': '0.360215'}]\n",
      "<class 'list'>\n",
      "no\n",
      "------------------\n",
      "0.332214\n",
      "<class 'numpy.float64'>\n",
      "no\n",
      "------------------\n",
      "Somewhat-Bullish\n",
      "<class 'str'>\n",
      "yes\n",
      "------------------\n",
      "[{'ticker': 'NFLX', 'relevance_score': '0.436009', 'ticker_sentiment_score': '0.357976', 'ticker_sentiment_label': 'Bullish'}, {'ticker': 'GOOG', 'relevance_score': '0.751437', 'ticker_sentiment_score': '0.466083', 'ticker_sentiment_label': 'Bullish'}, {'ticker': 'META', 'relevance_score': '0.751437', 'ticker_sentiment_score': '0.466083', 'ticker_sentiment_label': 'Bullish'}, {'ticker': 'AAPL', 'relevance_score': '0.436009', 'ticker_sentiment_score': '0.357976', 'ticker_sentiment_label': 'Bullish'}]\n",
      "<class 'list'>\n",
      "no\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_71116\\2626120922.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(news.iloc[0][i])\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_71116\\2626120922.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(type(news.iloc[0][i]))\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_71116\\2626120922.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if isinstance(news.iloc[0][i], str):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(news.iloc[0])):\n",
    "    print(news.iloc[0][i])\n",
    "    print(type(news.iloc[0][i]))\n",
    "\n",
    "    if isinstance(news.iloc[0][i], str):\n",
    "        print('yes')\n",
    "    else:\n",
    "        print('no')\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "fad83143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_time_before_published(time_str):\n",
    "    # Convert string to datetime\n",
    "    dt = datetime.strptime(time_str, '%Y%m%dT%H%M%S').replace(tzinfo=timezone.utc)\n",
    "\n",
    "    # Round down to the previous 10-minute mark\n",
    "    minutes = (dt.minute // 10) * 10\n",
    "    rounded = dt.replace(minute=minutes, second=0, microsecond=0)\n",
    "\n",
    "    # Define valid trading window\n",
    "    start_time = rounded.replace(hour=13, minute=30)\n",
    "    end_time = rounded.replace(hour=19, minute=50)\n",
    "\n",
    "    if start_time <= rounded <= end_time:\n",
    "        return rounded.isoformat(sep=' ')\n",
    "    elif rounded < start_time:\n",
    "        # Move to the previous day at 13:30 UTC\n",
    "        previous_day = (dt - timedelta(days=1)).replace(hour=19, minute=50, second=0, microsecond=0)\n",
    "        return previous_day.isoformat(sep=' ')\n",
    "    else:\n",
    "        # Move to the next day at 13:30 UTC\n",
    "        next_day = (dt + timedelta(days=1)).replace(hour=13, minute=30, second=0, microsecond=0)\n",
    "        return next_day.isoformat(sep=' ')\n",
    "\n",
    "\n",
    "def get_relevance(entry, ticker=\"AAPL\"):\n",
    "    if isinstance(entry, list):\n",
    "        for item in entry:\n",
    "            if item.get(\"ticker\") == ticker:\n",
    "                return float(item.get(\"relevance_score\", 0))\n",
    "    return None  # or 0.0 if you prefer\n",
    "\n",
    "def get_highest_scored_topic(entry):\n",
    "    if isinstance(entry, list):\n",
    "        # Sort the list of dictionaries by relevance_score in descending order\n",
    "        sorted_topics = sorted(entry, key=lambda x: float(x.get('relevance_score', 0)), reverse=True)\n",
    "        # Return the topic of the highest scored entry\n",
    "        return sorted_topics[0].get('topic', '') if sorted_topics else None\n",
    "    return None  # or \"\" if you'd prefer an empty string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "06f7f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'topics' column\n",
    "news[\"topic\"] = news[\"topics\"].apply(get_highest_scored_topic)\n",
    "news[\"relevance_score\"] = news[\"ticker_sentiment\"].apply(lambda x: get_relevance(x, ticker=\"AAPL\"))\n",
    "news[\"closest_time_before_published\"] = news[\"time_published\"].apply(get_closest_time_before_published)\n",
    "news['authors'] = news['authors'].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else '')\n",
    "news = news.drop(columns=[\"topics\"])\n",
    "news = news.drop(columns=[\"ticker_sentiment\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "97a55632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>time_published</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>banner_image</th>\n",
       "      <th>source</th>\n",
       "      <th>category_within_source</th>\n",
       "      <th>source_domain</th>\n",
       "      <th>overall_sentiment_score</th>\n",
       "      <th>overall_sentiment_label</th>\n",
       "      <th>topic</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>closest_time_before_published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Year Big Tech Stocks Fell From Glory</td>\n",
       "      <td>https://www.wsj.com/articles/the-year-big-tech...</td>\n",
       "      <td>20230101T103000</td>\n",
       "      <td>Hannah Miao</td>\n",
       "      <td>With interest rates rising sharply, momentum s...</td>\n",
       "      <td>https://images.wsj.net/im-695150/social</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>Markets</td>\n",
       "      <td>www.wsj.com</td>\n",
       "      <td>0.332214</td>\n",
       "      <td>Somewhat-Bullish</td>\n",
       "      <td>Technology</td>\n",
       "      <td>0.436009</td>\n",
       "      <td>2022-12-31 19:50:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 Beaten-Down Warren Buffett Stocks to Buy in ...</td>\n",
       "      <td>https://www.fool.com/investing/2023/01/01/beat...</td>\n",
       "      <td>20230101T101300</td>\n",
       "      <td>Cory Renauer</td>\n",
       "      <td>These are high-conviction stocks for the Oracl...</td>\n",
       "      <td>https://g.foolcdn.com/image/?url=https%3A%2F%2...</td>\n",
       "      <td>Motley Fool</td>\n",
       "      <td>n/a</td>\n",
       "      <td>www.fool.com</td>\n",
       "      <td>0.271653</td>\n",
       "      <td>Somewhat-Bullish</td>\n",
       "      <td>Earnings</td>\n",
       "      <td>0.277118</td>\n",
       "      <td>2022-12-31 19:50:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80% of Warren Buffett's Portfolio Is Invested ...</td>\n",
       "      <td>https://www.fool.com/investing/2023/01/01/80-o...</td>\n",
       "      <td>20230101T105000</td>\n",
       "      <td>Keith Speights</td>\n",
       "      <td>Two of these stocks were especially big winner...</td>\n",
       "      <td>https://g.foolcdn.com/image/?url=https%3A%2F%2...</td>\n",
       "      <td>Motley Fool</td>\n",
       "      <td>n/a</td>\n",
       "      <td>www.fool.com</td>\n",
       "      <td>0.232526</td>\n",
       "      <td>Somewhat-Bullish</td>\n",
       "      <td>Financial Markets</td>\n",
       "      <td>0.178021</td>\n",
       "      <td>2022-12-31 19:50:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0           The Year Big Tech Stocks Fell From Glory   \n",
       "1  2 Beaten-Down Warren Buffett Stocks to Buy in ...   \n",
       "2  80% of Warren Buffett's Portfolio Is Invested ...   \n",
       "\n",
       "                                                 url   time_published  \\\n",
       "0  https://www.wsj.com/articles/the-year-big-tech...  20230101T103000   \n",
       "1  https://www.fool.com/investing/2023/01/01/beat...  20230101T101300   \n",
       "2  https://www.fool.com/investing/2023/01/01/80-o...  20230101T105000   \n",
       "\n",
       "          authors                                            summary  \\\n",
       "0     Hannah Miao  With interest rates rising sharply, momentum s...   \n",
       "1    Cory Renauer  These are high-conviction stocks for the Oracl...   \n",
       "2  Keith Speights  Two of these stocks were especially big winner...   \n",
       "\n",
       "                                        banner_image               source  \\\n",
       "0            https://images.wsj.net/im-695150/social  Wall Street Journal   \n",
       "1  https://g.foolcdn.com/image/?url=https%3A%2F%2...          Motley Fool   \n",
       "2  https://g.foolcdn.com/image/?url=https%3A%2F%2...          Motley Fool   \n",
       "\n",
       "  category_within_source source_domain  overall_sentiment_score  \\\n",
       "0                Markets   www.wsj.com                 0.332214   \n",
       "1                    n/a  www.fool.com                 0.271653   \n",
       "2                    n/a  www.fool.com                 0.232526   \n",
       "\n",
       "  overall_sentiment_label              topic  relevance_score  \\\n",
       "0        Somewhat-Bullish         Technology         0.436009   \n",
       "1        Somewhat-Bullish           Earnings         0.277118   \n",
       "2        Somewhat-Bullish  Financial Markets         0.178021   \n",
       "\n",
       "  closest_time_before_published  \n",
       "0     2022-12-31 19:50:00+00:00  \n",
       "1     2022-12-31 19:50:00+00:00  \n",
       "2     2022-12-31 19:50:00+00:00  "
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first element in the 'authors' list\n",
    "news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "dd75dac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_71116\\1537425571.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  news.iloc[0][9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.332214)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.iloc[0][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "5c239cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_data(apikey='R6WUON3CB6JRP1K7', ticker='AAPL', from_date='2023-05-01', to_date='2025-05-01', limit=1000):\n",
    "    # keys lol: IG5JCBZ47L987AB9, R6WUON3CB6JRP1K7: free to get at alphavantage.co\n",
    "    start_date = datetime.strptime(from_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(to_date, '%Y-%m-%d')\n",
    "    current_date = start_date\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=365)\n",
    "        try:\n",
    "            print(f\"{current_date.strftime('%Y-%m-%d')} - {next_date.strftime('%Y-%m-%d')}\")\n",
    "            alpha_news = get_alpha_vantage_news(apikey=apikey, ticker=ticker, from_date=current_date.strftime('%Y-%m-%d'), to_date=next_date.strftime('%Y-%m-%d'), limit=limit)\n",
    "            print(f\"{len(alpha_news['feed'])} news articles fetched from {current_date.strftime('%Y-%m-%d')}\")\n",
    "            # Add your logic here, e.g., calling a function with these dates\n",
    "            news = pd.DataFrame([alpha_news[\"feed\"][0]])\n",
    "            for i in range(1, len(alpha_news[\"feed\"])):\n",
    "                news1 = pd.DataFrame([alpha_news[\"feed\"][i]])\n",
    "                news = pd.concat([news, news1], ignore_index=True)\n",
    "\n",
    "                # Clean dataframe\n",
    "            news[\"topic\"] = news[\"topics\"].apply(get_highest_scored_topic)\n",
    "            news[\"relevance_score\"] = news[\"ticker_sentiment\"].apply(lambda x: get_relevance(x, ticker=\"AAPL\"))\n",
    "            news[\"closest_time_before_published\"] = news[\"time_published\"].apply(get_closest_time_before_published)\n",
    "            news['authors'] = news['authors'].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else '')\n",
    "            news = news.drop(columns=[\"topics\"])\n",
    "            news = news.drop(columns=[\"ticker_sentiment\"])\n",
    "            news[\"ticker\"] = ticker\n",
    "            insert_NewsData(ticker, news, conn)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing news between {current_date.strftime('%Y-%m-%d')} - {next_date.strftime('%Y-%m-%d')}: {e}\")\n",
    "            print(alpha_news)    \n",
    "\n",
    "        current_date = next_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f79884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-02 - 2024-05-01\n",
      "❌ Error processing news between 2023-05-02 - 2024-05-01: 'feed'\n",
      "{'Information': 'We have detected your API key as WCL5ES1EON307LZA and our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}\n",
      "2024-05-01 - 2025-05-01\n",
      "❌ Error processing news between 2024-05-01 - 2025-05-01: 'feed'\n",
      "{'Information': 'We have detected your API key as WCL5ES1EON307LZA and our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}\n"
     ]
    }
   ],
   "source": [
    "get_news_data(apikey='R6WUON3CB6JRP1K7', ticker='AAPL', from_date='2023-05-02', to_date='2025-04-29', limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "76b288fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_connection(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "09e24b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_news = get_alpha_vantage_news(apikey=\"R6WUON3CB6JRP1K7\", ticker=\"AAPL\", from_date=\"2023-05-13\", to_date=\"2023-05-14\", limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "9db055a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Information': 'We have detected your API key as R6WUON3CB6JRP1K7 and our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_news"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
