{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da39c3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\andre\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\andre\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a35ead2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------+-------+-------+-------+\n",
      "|               date|   open|   high|    low|  close| volume|\n",
      "+-------------------+-------+-------+-------+-------+-------+\n",
      "|2023-04-24 15:30:00|164.975| 165.24|164.415| 165.06|60755.0|\n",
      "|2023-04-24 15:40:00| 165.04| 165.46| 164.94| 165.13|14699.0|\n",
      "|2023-04-24 15:50:00| 165.13|165.415|165.075| 165.17| 6410.0|\n",
      "|2023-04-24 16:00:00|165.175| 165.18| 164.82| 164.97|11225.0|\n",
      "|2023-04-24 16:10:00|164.955| 165.28| 164.82| 165.26|15571.0|\n",
      "|2023-04-24 16:20:00| 165.26|165.425|165.165|165.295|10654.0|\n",
      "|2023-04-24 16:30:00|165.285| 165.36| 164.47|  164.5| 8380.0|\n",
      "|2023-04-24 16:40:00|164.495| 164.52|164.165| 164.25|12247.0|\n",
      "|2023-04-24 16:50:00| 164.25|  164.4| 164.02|164.035|13455.0|\n",
      "|2023-04-24 17:00:00| 164.04| 164.23|  163.9| 164.15| 8376.0|\n",
      "|2023-04-24 17:10:00|164.145| 164.38| 164.07|164.355| 8748.0|\n",
      "|2023-04-24 17:20:00| 164.36| 164.41| 164.15| 164.17| 6981.0|\n",
      "|2023-04-24 17:30:00| 164.17| 164.33|  164.1|164.135|12273.0|\n",
      "|2023-04-24 17:40:00|164.135| 164.32| 163.99| 164.15|13159.0|\n",
      "|2023-04-24 17:50:00|164.145| 164.21|  164.0| 164.14| 7993.0|\n",
      "|2023-04-24 18:00:00| 164.14| 164.31| 164.09|164.105| 8317.0|\n",
      "|2023-04-24 18:10:00| 164.11|164.245| 164.04|164.245| 7918.0|\n",
      "|2023-04-24 18:20:00|164.245| 164.35| 164.21|164.325|12346.0|\n",
      "|2023-04-24 18:30:00|164.325|164.605|164.285|164.605| 5837.0|\n",
      "|2023-04-24 18:40:00|164.605| 164.62|164.395| 164.49| 4305.0|\n",
      "+-------------------+-------+-------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSV Load Example\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Load CSV file into DataFrame\n",
    "df = spark.read.csv(\"C:/Users/andre/OneDrive/Dokument/vscode/Examens/aapl_10min_2yr.csv\", header=True, inferSchema=True)\n",
    "# Show the first few rows\n",
    "df.show()\n",
    "\n",
    "# Print the schema (column types)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ad22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e98dc",
   "metadata": {},
   "source": [
    "checking a model ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28aac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASRecBERT(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len, hidden_size=128, num_heads=2, dropout_rate=0.1, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding_projection = nn.Linear(embedding_dim, hidden_size)\n",
    "        self.pos_emb = nn.Embedding(max_len, hidden_size)\n",
    "\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_size,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_size * 4,\n",
    "                dropout=dropout_rate,\n",
    "                batch_first=True,\n",
    "                activation=\"relu\"\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(nn.Sequential(*self.attention_layers), num_layers=num_layers)\n",
    "\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def _normalize_time_weights(self, time_weights, eps=1e-8):\n",
    "        \"\"\"Normalize nonzero time weights to [0.1, 1.0], keeping padding values untouched.\"\"\"\n",
    "        weights = time_weights.clone()\n",
    "        non_zero_mask = weights > 0\n",
    "        if non_zero_mask.any():\n",
    "            non_zero_weights = weights[non_zero_mask]\n",
    "            min_val = non_zero_weights.min()\n",
    "            max_val = non_zero_weights.max()\n",
    "            if min_val != max_val:\n",
    "                weights[non_zero_mask] = (non_zero_weights - min_val) / (max_val - min_val) * 0.9 + 0.1\n",
    "        weights[weights == 0] = eps  # Prevent zero masking issues\n",
    "        return weights\n",
    "    \n",
    "    def forward(self, sequence, time_weights):\n",
    "        batch_size, seq_len, _ = sequence.shape\n",
    "        device = sequence.device\n",
    "\n",
    "        # Compute padding mask: True for padding positions\n",
    "        padding_mask = (time_weights == 0).bool()\n",
    "\n",
    "        # Normalize time weights (ensuring non-zero values)\n",
    "        norm_time_weights = self._normalize_time_weights(time_weights)\n",
    "\n",
    "        # Project input embeddings\n",
    "        x = self.embedding_projection(sequence)\n",
    "\n",
    "        # Add positional encodings\n",
    "        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        pos_emb = self.pos_emb(positions)\n",
    "        x = x + pos_emb  # [B, L, H]\n",
    "\n",
    "        # Scale embeddings using normalized time weights\n",
    "        x = x * norm_time_weights.unsqueeze(-1)\n",
    "\n",
    "        # Pass through transformer with padding mask\n",
    "        x = self.transformer(x, src_key_padding_mask=padding_mask)\n",
    "\n",
    "        # Extract last non-padded token\n",
    "        last_indices = torch.sum(time_weights > 0, dim=1) - 1\n",
    "        last_indices = torch.clamp(last_indices, min=0)\n",
    "        batch_indices = torch.arange(batch_size, device=device)\n",
    "        final = x[batch_indices, last_indices]  # [B, H]\n",
    "\n",
    "        return self.output_projection(final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
